{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating PowerAI Vision for Sports Advertising\n",
    "\n",
    "### Overview\n",
    "This notebook demonstrates how to leverage a model built with PowerAI Vision to track logos at a sporting event and create a video showing both the logos detected within the video as well as keep track of the total time a given logo has been on the screen. PowerAI Vision can do object detection with rectangular boxes (bounding boxes for the objects detected) as well as polygons to detect the edges around the object. In this lab we will use boxes around the advertising logos and we will use a polygon to trace the outside of the goal keeper and the referee. \n",
    "\n",
    "This example is a soccer game where we will track the following logos:\n",
    "\n",
    "- fly erirates\n",
    "- toyota\n",
    "- kirin\n",
    "- continental\n",
    "\n",
    "as well as the ball.\n",
    "\n",
    "You can modify the example to use any video you like that has an object detection model deployed in PowerAI Vision. Comments throughout the notebook will help you modify it to your needs.\n",
    "\n",
    "Special thanks to Michael Hollinger for the initial code and his heavy lifting to create this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "# Import the modules we are going to use throughtout the code example.\n",
    "import cv2\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import signal\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 2\n",
    "\n",
    "# Check what version of cv2 we have installed - it should be 3.x \n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This next section contains all of the variables that you will need to change to customize this example for the model you trained and deployed in PowerAI Vision.\n",
    "\n",
    "The first section is just a set of boolean variables to determine if you want to perform all the functions in this notebook or just a subset of them (break the input video into separate frame files, render the images with times and labels, print out additional debug messages and clean up afterwards).\n",
    "\n",
    "The second section asks for the input video that you want to perform the object detection on and the name of an output video file you want to have rendered.\n",
    "\n",
    "The next section is the labels that you are using for object detection. These are the labels that you created in your PowerAI Vision training run. We also assign a specific color to each label for what will appear on the screen.\n",
    "\n",
    "Finally, we need the PowerAI Vision REST API endpoint that you have deployed the model to and the API ID that was assigned by PowerAI Vision when you deployed the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "EXPLODE_FRAMES=True            # If True then we will extract the video into jpeg frames\n",
    "DO_RENDERING=True              # If True then we will redraw the images with the labels and times in the image\n",
    "DEBUG=True                     # Print out some additional debug code\n",
    "CLEANUP_ORIGINAL_FRAMES=True   # After the video is recreated, do you want the original frames deleted\n",
    "CLEANUP_INFERRED_FRAMES=True   # After the video is recreated, do you want the labeled frames deleted\n",
    "\n",
    "\n",
    "videoclip = \"Japan_vs_Australia_Test.mp4\"     # This is the video file you want to use for input \n",
    "outputclip = \"OUTPUT_Japan_vs_Australia_Test.mp4\"    # This is the name of the output file with objects detected\n",
    "\n",
    "\n",
    "# Number of advertisers you are labeling\n",
    "num_ads=5\n",
    "\n",
    "# Set the advertisers you are labelling and their associated colors\n",
    "colortable={\n",
    "            #advertisers\n",
    "            'fly emirates': (33,26,215),\n",
    "            'toyota': (254,0,0),\n",
    "            'kirin':  (0,209,254),\n",
    "            'continental':(6,122,10),\n",
    "            'ball':(63,0,102)\n",
    "           }\n",
    "\n",
    "# define the logos you want to fill with polygons\n",
    "polyfill = {'ball'}\n",
    "\n",
    "# Values for PowerAI REST API\n",
    "pai_api = 'api/dlapis/4807904b-cf00-442c-a716-9f24907a6bee'\n",
    "powerai_baseurl='https://10.31.204.151/powerai-vision/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this next section we take the input video and break it down into individual frames and store each frame as a jpeg file. We will then use those files as input to the PowerAI Vision rest API to perform object detection on each frame.\n",
    "\n",
    "We are using cv2 to get individual frames and then we write those frames out to a jpeg file into format frameN.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded!\n",
      "Video is 854 x 420\n",
      "Video frame rate is 30\n",
      "Frame count estimate is 976\n",
      "20.0 frames processed\n",
      "40.0 frames processed\n",
      "60.0 frames processed\n",
      "80.0 frames processed\n",
      "100.0 frames processed\n",
      "120.0 frames processed\n",
      "140.0 frames processed\n",
      "160.0 frames processed\n",
      "180.0 frames processed\n",
      "200.0 frames processed\n",
      "220.0 frames processed\n",
      "240.0 frames processed\n",
      "260.0 frames processed\n",
      "280.0 frames processed\n",
      "300.0 frames processed\n",
      "320.0 frames processed\n",
      "340.0 frames processed\n",
      "360.0 frames processed\n",
      "380.0 frames processed\n",
      "400.0 frames processed\n",
      "420.0 frames processed\n",
      "440.0 frames processed\n",
      "460.0 frames processed\n",
      "480.0 frames processed\n",
      "500.0 frames processed\n",
      "520.0 frames processed\n",
      "540.0 frames processed\n",
      "560.0 frames processed\n",
      "580.0 frames processed\n",
      "600.0 frames processed\n",
      "620.0 frames processed\n",
      "640.0 frames processed\n",
      "660.0 frames processed\n",
      "680.0 frames processed\n",
      "700.0 frames processed\n",
      "720.0 frames processed\n",
      "740.0 frames processed\n",
      "760.0 frames processed\n",
      "780.0 frames processed\n",
      "800.0 frames processed\n",
      "820.0 frames processed\n",
      "840.0 frames processed\n",
      "860.0 frames processed\n",
      "880.0 frames processed\n",
      "900.0 frames processed\n",
      "920.0 frames processed\n",
      "940.0 frames processed\n",
      "960.0 frames processed\n",
      "Last frame complete at 976\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "\n",
    "def signal_handler(signal, frame):\n",
    "    # KeyboardInterrupt detected, exiting\n",
    "    global is_interrupted\n",
    "    is_interrupted = True\n",
    "    \n",
    "\n",
    "# First let's turn the video into a set of JPEG files\n",
    "if EXPLODE_FRAMES:\n",
    "    videofile = Path(videoclip)\n",
    "    if videofile.is_file():\n",
    "        cap = cv2.VideoCapture(videoclip)\n",
    "        while not cap.isOpened():\n",
    "            cap = cv2.VideoCapture(videoclip)\n",
    "            cv2.waitKey(1000)\n",
    "            print (\"Wait for the header\")\n",
    "        print (\"File loaded!\")\n",
    "    else:\n",
    "        print (\"File doesn't exist!\")\n",
    "\n",
    "    \n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    is_interrupted = False\n",
    "    # get the video size for later rendering and screen placement of the advertising times\n",
    "    vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))-60\n",
    "    vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    if DEBUG:\n",
    "        print(\"Video is %d x %d\" % (vid_width,vid_height))\n",
    "        print(\"Video frame rate is %d\" % FPS)\n",
    "        print(\"Frame count estimate is %d\" % cap.get(cv2.CAP_PROP_FRAME_COUNT))  # also print the number of frames in the video\n",
    "    \n",
    "    pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    fail_count = 0;\n",
    "    while True:\n",
    "        flag, frame = cap.read()\n",
    "        frame = frame[0:vid_height,:]\n",
    "                    \n",
    "        if flag:\n",
    "            pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            name = \"frame%d.jpg\"%pos_frame\n",
    "            cv2.imwrite(name, frame)\n",
    "\n",
    "            #let's only print out every 20 frames to keep the output down for long videos\n",
    "            if DEBUG:\n",
    "                if pos_frame % 20 == 0:\n",
    "                    print (str(pos_frame)+\" frames processed\")\n",
    "        else:\n",
    "            # The next frame is not ready, so we try to read it again\n",
    "            print (\"frame is not ready, retrying at frame %d\" % pos_frame)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame-1)\n",
    "            fail_count += 1\n",
    "            if fail_count == 5:\n",
    "                print(\"Too many failures, exiting at frame %d\" % pos_frame)\n",
    "                break\n",
    " \n",
    "        if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "            # If the number of captured frames is equal to the total number of frames, we stop\n",
    "            print (\"Last frame complete at %d\" % pos_frame)\n",
    "            break\n",
    "        \n",
    "# otherwise we already have the JPEG files so skip the exploding\n",
    "else:\n",
    "    print(\"Skipping frame explode...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This next section is actually calling the deployed model to perform the object detection and return the caller the JSON output of the object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "#define the code to do an inference with IBM PowerAI Vision\n",
    "def inferImage(img, api_id):\n",
    "    endpoint=powerai_baseurl+api_id\n",
    "    f = open(img, 'rb')\n",
    "    myfiles={'files': (img, f)}\n",
    "    rc = 0\n",
    "    retry_count = 0\n",
    "    while (rc != 200) and (retry_count < 5):\n",
    "        if retry_count != 0:\n",
    "            print (\"retrying upload for %s, attempt %d\" % (img, retry_count))\n",
    "        r = requests.post(endpoint, files=myfiles, verify=False)\n",
    "        rc = r.status_code\n",
    "        retry_count = retry_count + 1\n",
    "    f.close()\n",
    "    resp_value=json.loads(r.text)\n",
    "    # print( resp_value )\n",
    "    return rc, resp_value['classified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Got back 0 objects\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "\n",
    "# infer from just one frame to make sure we get what we expect - note that if the first frame of the test video\n",
    "# does not have any objects in it, you will see \"Got back 0 objects\". Otherwise you will see the objects detected\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "rc, objresp = inferImage('frame1.jpg',pai_api)\n",
    "if rc == 200:\n",
    "    print (json.dumps(objresp, indent=2))\n",
    "    print ( \"Got back %d objects\" % len(objresp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "def drawText(image, text, x, y, size, color, font):\n",
    "    cv2.putText(image, text, (x, y), font, size, color,2,cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next section takes the items that were detected and performs two things \n",
    "\n",
    "1) update_metrics will increment the counters for the various labels so that we can know how long each logo was in the video. It will also draw a grey box over top of the video image and print out the total time each logo has been on the screen up to this point.\n",
    "\n",
    "2) draw_label will put the coloured dot in the midpoint of the box where the object was detected in the inference stage and also write the label of the logo next to that dot.\n",
    "\n",
    "3) if we find one of the objects that you want to fill in with segmentation then we will fill in that using cv2 to fill in the polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Define the code that will put the labels onto of the images and add the total count times\n",
    "ads_drawn = defaultdict(int)\n",
    "\n",
    "# This is the location on the screen where the ad times will go - if you want to move it to the right increase the AD_START_X\n",
    "AD_START_X = 25\n",
    "AD_START_Y = vid_height - (num_ads * 25 + 50) # the height will depend on the number of ads we are tracking\n",
    "\n",
    "AD_BOX_COLOR=(180,160,160)  # Make the ad timing box grey\n",
    "COLOR_WHITE=(255,255,255)   # Make the text of the labels and the title white\n",
    "\n",
    "# This is flushed after each frame is drawn, but we use it to keep track of what's on-screen at a given moment\n",
    "things_drawn = defaultdict(int)\n",
    "\n",
    "# This is the code that will count up the ad times on the screen\n",
    "def update_metrics(image):\n",
    "    \n",
    "    # Count that this frame had an object of interest, regardless of the number of those items...\n",
    "    for thing, count in things_drawn.items():\n",
    "        # if DEBUG:\n",
    "            # print(\"Found %d counts of %s\" % (count, thing))  # even in debug mode this can print out too much so commented out\n",
    "        ads_drawn[thing]+=1\n",
    "    \n",
    "    #flush the things-drawn\n",
    "    things_drawn.clear()\n",
    "    \n",
    "    \n",
    "    # Make an overlay with the image shaded the way we want it...\n",
    "    overlay = image.copy()\n",
    "\n",
    "    # Shade Ad Box\n",
    "    cv2.rectangle(overlay,\n",
    "                  (AD_START_X-15, AD_START_Y-25),\n",
    "                  (AD_START_X+355, AD_START_Y+(num_ads*25)+30), AD_BOX_COLOR, cv2.FILLED)\n",
    "    cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)\n",
    "    \n",
    "    #draw all labels in the ad table\n",
    "    cursor_x = AD_START_X\n",
    "    cursor_y = AD_START_Y\n",
    "    #draw heading\n",
    "    drawText(image=image,\n",
    "             text=\"Advertiser Screen Time (sec)\",\n",
    "             x=cursor_x,\n",
    "             y=cursor_y,\n",
    "             size=0.7,\n",
    "             color=COLOR_WHITE,\n",
    "             font=cv2.FONT_HERSHEY_SIMPLEX)\n",
    "    #draw ad labels\n",
    "    for ad_label in sorted(ads_drawn, key=ads_drawn.get, reverse=True):\n",
    "        if not ad_label in polyfill:\n",
    "            screen_time = ads_drawn[ad_label]\n",
    "            cursor_y += 25\n",
    "            FONT_SCALE=0.7\n",
    "            col = colortable[ad_label] if ad_label in colortable else (255,255,255)\n",
    "            screen_time = screen_time/(FPS*1.0)\n",
    "        \n",
    "        \n",
    "            drawText(image=image,\n",
    "                     text=ad_label,\n",
    "                     x=cursor_x,\n",
    "                     y=cursor_y,\n",
    "                     size=0.6,\n",
    "                     color=col,\n",
    "                     font = cv2.FONT_HERSHEY_SIMPLEX)\n",
    "            drawText(image=image,\n",
    "                     text=\":\",\n",
    "                     x=cursor_x + 130,\n",
    "                     y=cursor_y,\n",
    "                     size=0.6,\n",
    "                     color=COLOR_WHITE,\n",
    "                     font = cv2.FONT_HERSHEY_SIMPLEX)\n",
    "            drawText(image=image,\n",
    "                     text=\"{0:.02f}\".format(screen_time), #render fractions of a second\n",
    "                     x=cursor_x + 140,\n",
    "                     y=cursor_y,\n",
    "                     size=0.6,\n",
    "                     color=COLOR_WHITE,\n",
    "                     font = cv2.FONT_HERSHEY_SIMPLEX)\n",
    "    \n",
    "    return\n",
    "\n",
    "# in order to avoid the jitter of the ojbects detected points in the final video we will round to the nearest 15 pixels\n",
    "def myround(x, base=15):\n",
    "    return int(base * round(float(x)/base))\n",
    "\n",
    "# this will draw a colored dot and the name of the label on top of the object we are detecting\n",
    "def draw_label(resp, image):\n",
    "    name = resp.get(\"label\", \"\")\n",
    "    col = colortable[name]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontface = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontscale = 1\n",
    "    thickness = 1\n",
    "    fontcolor = (255,255,255)\n",
    "    textSize, baseline = cv2.getTextSize(name, fontface,\n",
    "                                fontscale, thickness);\n",
    "    \n",
    "    # things_drawn keeps track of how many times we have seen this ad (indexed by ad name)\n",
    "    things_drawn[name]+=1\n",
    "\n",
    "    xmin = resp.get(\"xmin\", \"\")\n",
    "    xmax = resp.get(\"xmax\", \"\")\n",
    "    ymin = resp.get(\"ymin\", \"\")\n",
    "    ymax = resp.get(\"ymax\", \"\")\n",
    "    xmid = int(myround((xmin+xmax)/2))\n",
    "    ymid = int(myround((ymin+ymax)/2))\n",
    "    \n",
    "    # this next line will draw the polygon around the object\n",
    "    #   cv2.polylines(image, [np.int32(obj['polygons'][0])], True, color=col, thickness=2, lineType=8, shift=0)\n",
    "    # but for our purposes if it's not the referee or the keeper, let's just put a dot on the advertiser and the name\n",
    "    cv2.circle(image, (xmid, ymid), 8, color=col, thickness=-1, lineType=8, shift=0)\n",
    "    cv2.putText(image, name, (xmid+5, ymid+(textSize[1])), font, 0.9,(255,255,255),2,cv2.LINE_AA)\n",
    "    \n",
    "    if name in polyfill:\n",
    "        overlay = image.copy()\n",
    "        cv2.fillPoly(overlay, [np.int32(obj['polygons'][0])], col)\n",
    "        cv2.addWeighted(overlay, .5, image, .5, 0, image)\n",
    "    \n",
    "    cv2.putText(image, name, (xmid+5, ymid+(textSize[1])), font, 0.9,(255,255,255),2,cv2.LINE_AA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the PowerAI Inferencing engine for each frame\n",
    "\n",
    "Now for each frame we call the PowerAI Vision REST API with the jpg image of that frame for it to give us back the objects found within the frame. Note that the first part of the code just steps through each frame one by one to call the inference API. But for a long video this can be slow, so the next section of the code creates N threads, each calling the inference engine for 1/Nth of the frames. You can adjust the number of threads based on what client you are running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling PowerAI Inference in 6 threads...(wait for all threads to complete before executing next cell)\n",
      "327,  1,  490,  653,  164,  816,  328,  2,  491,  654,  165,  817,  329,  3,  492,  655,  166,  330,  818,  4,  493,  656,  331,  167,  819,  5,  494,  657,  332,  168,  820,  6,  495,  658,  333,  169,  821,  7,  496,  334,  659,  170,  822,  8,  497,  335,  660,  823,  9,  336,  498,  661,  824,  171,  499,  825,  662,  10,  337,  172,  500,  826,  663,  11,  338,  173,  501,  827,  12,  339,  664,  174,  502,  828,  13,  340,  665,  175,  503,  829,  341,  14,  666,  176,  504,  830,  342,  15,  667,  505,  177,  831,  343,  16,  668,  506,  178,  832,  344,  17,  669,  507,  179,  833,  345,  18,  508,  670,  180,  834,  346,  19,  509,  671,  181,  835,  347,  510,  20,  672,  182,  836,  511,  348,  21,  673,  183,  837,  512,  349,  22,  674,  184,  838,  513,  350,  23,  675,  185,  839,  514,  351,  24,  186,  840,  515,  25,  187,  352,  676,  841,  516,  26,  188,  353,  677,  842,  517,  27,  189,  354,  678,  843,  518,  28,  190,  355,  679,  844,  519,  29,  191,  356,  680,  845,  520,  30,  192,  357,  681,  846,  521,  31,  193,  358,  682,  847,  522,  32,  194,  359,  683,  848,  523,  33,  195,  360,  684,  849,  524,  34,  196,  361,  685,  850,  525,  35,  197,  362,  686,  526,  36,  198,  363,  687,  527,  37,  199,  364,  688,  528,  38,  200,  365,  689,  529,  39,  201,  851,  366,  690,  530,  40,  852,  367,  691,  531,  41,  853,  202,  368,  692,  532,  42,  854,  203,  369,  693,  533,  43,  855,  204,  370,  694,  534,  44,  856,  205,  371,  695,  535,  45,  857,  206,  372,  696,  536,  46,  858,  207,  373,  697,  537,  47,  859,  208,  374,  698,  538,  48,  860,  209,  375,  699,  539,  49,  861,  210,  376,  700,  540,  50,  862,  211,  377,  701,  541,  51,  863,  212,  378,  702,  542,  52,  864,  213,  379,  703,  543,  53,  865,  214,  380,  704,  544,  866,  54,  215,  381,  705,  545,  867,  55,  216,  382,  706,  546,  868,  217,  56,  383,  707,  547,  869,  218,  57,  384,  708,  548,  870,  219,  58,  385,  709,  549,  871,  220,  59,  386,  710,  550,  872,  221,  60,  387,  711,  551,  873,  61,  388,  712,  552,  874,  62,  389,  713,  553,  875,  63,  390,  714,  554,  876,  64,  391,  222,  715,  555,  877,  65,  392,  223,  716,  556,  878,  66,  224,  717,  557,  393,  879,  67,  718,  225,  558,  880,  68,  226,  559,  394,  881,  69,  719,  227,  560,  395,  882,  70,  720,  228,  396,  561,  883,  71,  721,  229,  397,  562,  884,  72,  722,  230,  398,  563,  885,  73,  723,  231,  399,  564,  886,  74,  724,  232,  400,  565,  887,  75,  725,  233,  401,  566,  888,  76,  726,  234,  402,  567,  889,  77,  727,  235,  403,  568,  890,  78,  728,  236,  404,  569,  891,  79,  729,  237,  405,  570,  892,  80,  730,  238,  406,  571,  893,  81,  731,  239,  407,  894,  572,  82,  732,  240,  408,  895,  573,  83,  733,  241,  896,  409,  574,  84,  734,  242,  897,  410,  575,  85,  735,  898,  243,  411,  576,  86,  736,  899,  244,  412,  577,  87,  737,  900,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-373191929aa1>\", line 36, in worker\n",
      "    rc, tracking_results[name] = inferImage(name,pai_api)\n",
      "  File \"<ipython-input-5-afc160849446>\", line 19, in inferImage\n",
      "    return rc, resp_value['classified']\n",
      "KeyError: 'classified'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413,  578,  88,  901,  738,  414,  579,  89,  902,  739,  415,  580,  903,  90,  740,  416,  581,  904,  91,  741,  417,  905,  582,  92,  742,  418,  906,  583,  93,  743,  419,  907,  584,  94,  744,  908,  420,  585,  95,  745,  421,  909,  586,  96,  746,  422,  910,  587,  97,  747,  911,  423,  588,  98,  424,  912,  589,  99,  425,  913,  590,  748,  100,  914,  426,  591,  749,  101,  915,  427,  592,  750,  102,  916,  428,  593,  751,  103,  917,  429,  594,  752,  918,  104,  430,  595,  753,  919,  105,  431,  596,  920,  754,  106,  432,  921,  597,  755,  433,  922,  598,  756,  107,  434,  923,  599,  757,  108,  435,  924,  600,  758,  109,  925,  436,  601,  759,  110,  926,  437,  602,  760,  111,  927,  438,  603,  761,  928,  112,  439,  604,  929,  762,  113,  440,  605,  930,  763,  114,  441,  606,  931,  764,  115,  442,  932,  607,  765,  116,  443,  933,  608,  766,  117,  934,  444,  609,  767,  118,  935,  445,  768,  610,  936,  119,  446,  769,  611,  937,  120,  770,  938,  612,  447,  121,  771,  939,  613,  448,  122,  772,  940,  614,  449,  123,  773,  941,  615,  450,  774,  124,  942,  616,  451,  775,  125,  943,  617,  776,  452,  126,  944,  618,  777,  453,  127,  945,  619,  778,  454,  128,  946,  620,  779,  455,  947,  129,  621,  780,  456,  948,  130,  622,  781,  949,  457,  131,  623,  782,  950,  458,  132,  624,  783,  951,  459,  133,  625,  784,  952,  460,  134,  626,  785,  953,  461,  135,  627,  954,  786,  462,  136,  955,  628,  787,  463,  137,  956,  629,  788,  464,  138,  957,  789,  630,  465,  958,  139,  790,  631,  466,  959,  140,  791,  632,  467,  960,  141,  792,  633,  961,  468,  142,  793,  962,  634,  469,  794,  143,  963,  635,  470,  795,  144,  964,  636,  796,  471,  145,  965,  637,  797,  472,  966,  146,  798,  638,  473,  967,  147,  799,  639,  474,  968,  148,  800,  640,  475,  969,  149,  801,  641,  476,  970,  150,  802,  642,  971,  477,  151,  803,  643,  972,  478,  152,  804,  644,  973,  479,  153,  805,  974,  645,  480,  154,  806,  975,  646,  481,  155,  807,  976,  482,  156,  647,  483,  157,  648,  484,  158,  649,  485,  159,  650,  486,  160,  808,  651,  487,  161,  809,  652,  488,  162,  810,  489,  163,  811,  812,  813,  814,  815,  All threads complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 \n",
    "\n",
    "#Now for each frame call PowerAI Vision to perform the inferencing\n",
    "tracking_results = {}\n",
    "\n",
    "# This section does the inference serialized. The inference REST endpoing in PowerAI Vision is called\n",
    "# and the results are stored in a \"tracking_results\" dict for later processing\n",
    "# It works fine but it's slow for processing large videos so the next section uses a set of threads to make more calls simultaneously\n",
    "#\n",
    "# i = 0;\n",
    "# for name in glob.glob('frame*.jpg'):\n",
    "#    rc, tracking_results[name] = inferImage(name,pai_api)\n",
    "#    \n",
    "#    i +=1\n",
    "#    print(\".\", end = \" \")\n",
    "#    # let's only print every 20 frames to keep the output down for long videos\n",
    "#    if i % 20 == 0:\n",
    "#        print (\"%d frames processed\" % i)\n",
    "#        \n",
    "# print(\"complete %d frames\" % i)\n",
    "\n",
    "# Serial is too slow so let's run the inferencing across a set of threads (each thread doing a 1/N of the images)\n",
    "import threading\n",
    "import math\n",
    "\n",
    "num_threads = 6\n",
    "# The worker thread will check to see what thread they are assigned to and pick 1/num_threads of the images based on that\n",
    "# Just for debugging we will print out the frame number that just finished an inference so we can make sure they are all moving\n",
    "def worker(num):\n",
    "    start_frame = int(math.ceil(pos_frame/num_threads))*num + 1;\n",
    "    end_frame = int(math.ceil(pos_frame/num_threads))*(num+1) + 1;\n",
    "    for i in range(start_frame, end_frame):\n",
    "            name = \"frame\" + str(i) + \".jpg\"\n",
    "            # check if the file exists and if so call the inference engine\n",
    "            if os. path. isfile(name):\n",
    "                rc, tracking_results[name] = inferImage(name,pai_api)\n",
    "                if DEBUG:\n",
    "                    print(\"%d, \" % i, end = \" \")        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "#Now spin up a bunch threads\n",
    "threads = []\n",
    "print(\"Calling PowerAI Inference in %d threads...(wait for all threads to complete before executing next cell)\" % num_threads)\n",
    "for i in range(num_threads):\n",
    "    t = threading.Thread(target=worker, args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# wait for all the threads to complete before going on to the next cell\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(\"All threads complete\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redraw the frames with the objects labeled and the time for each log displayed\n",
    "\n",
    "Now that all the code above is set up, go through each frame (in frame order) and call the function to update the metrics and draw the labels that were detected in the above inferencing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . 20 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 40 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 60 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 80 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 100 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 120 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 140 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 160 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 180 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 200 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 220 frames redrawn\n",
      ". . . . . . . . . . . . . . . . . . . . 240 frames redrawn\n",
      ". . . . . "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'frame245.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-009b6e0dad62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mjsonresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeyframe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDO_RENDERING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# first update the metrics for the image to add any new time for the ads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'frame245.jpg'"
     ]
    }
   ],
   "source": [
    "# Cell 10\n",
    "\n",
    "from shutil import copyfile\n",
    "import traceback\n",
    "\n",
    "# now we want to take the original frames and draw the inferences on top of them and then write out the new frame\n",
    "for i in range(1, len(glob.glob('frame*.jpg'))): \n",
    "    keyframe = \"frame\" + str(i) + \".jpg\"\n",
    "    name = \"frame\" + str(i) + \".jpg\"\n",
    "   \n",
    "    print(\".\", end = \" \")\n",
    "    # let's only print every 20 frames to keep the output down for long videos\n",
    "    if i % 20 == 0:\n",
    "        print (\"%d frames redrawn\" % i)\n",
    "        \n",
    "    if DO_RENDERING:\n",
    "        img = cv2.imread(name)\n",
    "    try:\n",
    "        jsonresp = tracking_results[keyframe]\n",
    "        if DO_RENDERING:\n",
    "            # first update the metrics for the image to add any new time for the ads\n",
    "            # and then draw the metric overlay onto the image\n",
    "            update_metrics(img)\n",
    "            \n",
    "            output = img.copy()\n",
    "            # Now also label all the objects we found in the frame\n",
    "            for obj in jsonresp:\n",
    "                draw_label(obj, output)\n",
    "           \n",
    "            # Write the new image to the output file which is the original image with \n",
    "            # the detected objects labeled and the sum of the ad time on the screen\n",
    "            cv2.imwrite(\"output-\"+name, output)\n",
    "            \n",
    "    except AssertionError as e:\n",
    "        print (\"Missing %s data\" % name)\n",
    "        traceback.print_exc()\n",
    "        copyfile(name, \"output-\"+name)\n",
    "        \n",
    "print(\"complete %d frames redrawn\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put the individual frames that have been redrawn back into a video\n",
    "\n",
    "Now that we have all the redrawn frames in individual jpeg images, we string them back together into a video. We are using FFMPEG to do this and we pass in the frame rate and the image files and the image size (that we got from the original video). You will need to update the path below to point to where you installed ffmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ...  done\n"
     ]
    }
   ],
   "source": [
    "# Cell 11\n",
    "\n",
    "# Now string all the jpeg files back into a video using ffmpeg\n",
    "from subprocess import call\n",
    "\n",
    "cmd_string = \"/usr/bin/ffmpeg -y -r \" + str(FPS) + \" -f image2 -s \" + str(vid_width) + \"x\" + str(vid_height) + \" -i ./output-frame%d.jpg  -pix_fmt yuv420p \" + str(outputclip)\n",
    "print(\"Processing ... \", end = \" \")\n",
    "call(cmd_string,shell=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "The last cell will delete all the individual frames if you have told it to. If you are debugging your video you may want to keep the input frames around and test your model with individual frames to see what confidence comes back and what objects are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing frameX.jpg\n",
      "removing output-frameX.jpg\n"
     ]
    }
   ],
   "source": [
    "# Cell 12\n",
    "\n",
    "# Clean up all the intermediate files if the variables are set to do so\n",
    "if CLEANUP_ORIGINAL_FRAMES:\n",
    "    print(\"removing frameX.jpg\")\n",
    "    for f1 in glob.glob(\"frame*.jpg\"):\n",
    "        os.remove(f1)\n",
    "    \n",
    "if CLEANUP_INFERRED_FRAMES:\n",
    "    print(\"removing output-frameX.jpg\")\n",
    "    for f2 in glob.glob(\"output-frame*.jpg\"):\n",
    "        os.remove(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test new video file\n",
    "\n",
    "Run the new video file you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"OUTPUT_Japan_vs_Australia_Test.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 13\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"OUTPUT_Japan_vs_Australia_Test.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
